{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**VGG16/19 to tflite model with integer weights**"
      ],
      "metadata": {
        "id": "k25bB6NmlWz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use tensorflow version 2.10 for our objective"
      ],
      "metadata": {
        "id": "gUK_8VGumFLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow==2.10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zxf8_ew1RUct",
        "outputId": "527ab1f2-5783-4f84-a870-1636327ea9fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.10\n",
            "  Downloading tensorflow-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.0/578.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (24.3.25)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.10)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (1.62.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (3.9.0)\n",
            "Collecting keras<2.11,>=2.10.0 (from tensorflow==2.10)\n",
            "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.1 (from tensorflow==2.10)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (24.0)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.10)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (1.16.0)\n",
            "Collecting tensorboard<2.11,>=2.10 (from tensorflow==2.10)\n",
            "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (0.36.0)\n",
            "Collecting tensorflow-estimator<2.11,>=2.10.0 (from tensorflow==2.10)\n",
            "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.10) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.2.2)\n",
            "Installing collected packages: tensorboard-plugin-wit, keras, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, gast, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.4\n",
            "    Uninstalling gast-0.5.4:\n",
            "      Successfully uninstalled gast-0.5.4\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.0\n",
            "    Uninstalling google-auth-oauthlib-1.2.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.19.6 which is incompatible.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.10.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "fe0052ef39814639a37ed0f1d42bcbea"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirming version of tensorflow"
      ],
      "metadata": {
        "id": "cfvhbcJPmXrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQdQRzluRyI7",
        "outputId": "91e79255-ca99-4094-c359-38d29c0709ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the model**"
      ],
      "metadata": {
        "id": "JaKMwigAmj8B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "noCttfKnxf0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07a13258-37a0-41fa-b79c-c37c4b869aa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels.h5\n",
            "574710816/574710816 [==============================] - 5s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "\n",
        "# Load the pre-trained VGG-16 model\n",
        "model = VGG19(weights='imagenet', include_top=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cloning this git repository containing images for validation**"
      ],
      "metadata": {
        "id": "7r0Fz1wzmpR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ndb796/Small-ImageNet-Validation-Dataset-1000-Classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBV9s1NnH5ff",
        "outputId": "0c90b382-da70-47c2-e3f5-f80709cba871"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Small-ImageNet-Validation-Dataset-1000-Classes'...\n",
            "remote: Enumerating objects: 6022, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 6022 (delta 1), reused 0 (delta 0), pack-reused 6016\u001b[K\n",
            "Receiving objects: 100% (6022/6022), 624.87 MiB | 40.12 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n",
            "Updating files: 100% (5005/5005), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tflite conversion**"
      ],
      "metadata": {
        "id": "bGV6Q9i0Flw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# Define the path to the directory containing images\n",
        "image_dir = '/content/Small-ImageNet-Validation-Dataset-1000-Classes/ILSVRC2012_img_val_subset'\n",
        "\n",
        "selected_dirs = random.sample(os.listdir(image_dir), 5)  # Choose 5 random directories\n",
        "\n",
        "# Defining the representative dataset function\n",
        "\n",
        "def representative_dataset():\n",
        "    # Iterate over each selected subdirectory\n",
        "  for subdir in selected_dirs:\n",
        "    subdir_path = os.path.join(image_dir, subdir)\n",
        "    if os.path.isdir(subdir_path):\n",
        "      # List all images in the subdirectory\n",
        "      images = os.listdir(subdir_path)\n",
        "      # Shuffle the list of images\n",
        "      random.shuffle(images)\n",
        "      # Process the first image in the shuffled list\n",
        "      image_name = images[0]\n",
        "      image_path = os.path.join(subdir_path, image_name)\n",
        "\n",
        "      # Load and preprocess the image\n",
        "      img = Image.open(image_path).resize((224, 224))\n",
        "      # Skip grayscale images\n",
        "      if len(np.array(img).shape) == 2:\n",
        "        continue\n",
        "      img_array = np.array(img)\n",
        "      # img_array = img_array.astype(np.uint8)\n",
        "      img_array = preprocess_input(img_array)  # Preprocess the input image\n",
        "      img_array = np.expand_dims(img_array, axis=0)\n",
        "      # Calculate statistics (assuming RGB image channels)\n",
        "      min_val = np.amin(img_array, axis=(0, 1, 2))  # Min value for each channel\n",
        "      max_val = np.amax(img_array, axis=(0, 1, 2))  # Max value for each channel\n",
        "      stats = [min_val[0], min_val[1], min_val[2], max_val[0], max_val[1], max_val[2]]\n",
        "      yield {\n",
        "          model.inputs[0].name: np.array(img_array,dtype=np.float32),  # Replace 'model.inputs[0].name' with your actual input name\n",
        "          'stats': np.array(stats)\n",
        "      }\n",
        "\n",
        "\n",
        "\n",
        "# Set up the TensorFlow Lite converter\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# converter.calibrate_inputs = True  # Enable calibration\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "# converter.experimental_new_quantizer = True  # Enable experimental quantization\n",
        "\n",
        "# Convert the model to TensorFlow Lite format\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "# Save the quantized TensorFlow Lite model to a file\n",
        "with open('vgg19_tfl.tflite', 'wb') as f:\n",
        "    f.write(tflite_quant_model)\n"
      ],
      "metadata": {
        "id": "p4kmwCoFEBDF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb66b3a3-a995-4344-d01d-d5a41386a08e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Uncomment this codeblock to check prediction of original model on an image**"
      ],
      "metadata": {
        "id": "ZwSJ-_wlnbMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.preprocessing import image\n",
        "# from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "# import numpy as np\n",
        "\n",
        "# # Load the image\n",
        "# img_path = 'Enter image path'\n",
        "# img = image.load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "# # Convert the image to a numpy array\n",
        "# img_array = image.img_to_array(img)\n",
        "\n",
        "# # Expand the dimensions to create a batch (VGG16 expects batches)\n",
        "# img_batch = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# # Preprocess the input (normalization, etc.)\n",
        "# img_batch = preprocess_input(img_batch)\n",
        "\n",
        "# # Make predictions\n",
        "# predictions = model.predict(img_batch)\n",
        "# print(np.argmax(predictions))\n",
        "# # print(predictions)\n",
        "# # Decode predictions (optional)\n",
        "# decoded_predictions = decode_predictions(predictions, top=3)[0]\n",
        "\n",
        "# # Print the top 3 predictions\n",
        "# for pred in decoded_predictions:\n",
        "#     print(pred)"
      ],
      "metadata": {
        "id": "QXZiJo1E_UCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction of tflite model on an image from validation set**"
      ],
      "metadata": {
        "id": "urXjBeGPoHoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Load the image\n",
        "img_path = \"/content/Small-ImageNet-Validation-Dataset-1000-Classes/ILSVRC2012_img_val_subset/319/ILSVRC2012_val_00000418.JPEG\"\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "# Convert the image to a numpy array\n",
        "img_array = image.img_to_array(img)\n",
        "\n",
        "# Expand the dimensions to create a batch (TFLite expects batches)\n",
        "img_batch = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# Preprocess the input\n",
        "img_batch = preprocess_input(img_batch)\n",
        "\n",
        "# Convert the input data to UINT8\n",
        "img_batch = (np.round((img_batch/1.0774157047271729)-13)-128).astype(np.uint8)\n",
        "\n",
        "# Load the TensorFlow Lite model\n",
        "interpreter = tf.lite.Interpreter(model_path='/content/vgg19_tfl.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Set the input tensor for the TensorFlow Lite model\n",
        "input_details = interpreter.get_input_details()\n",
        "interpreter.set_tensor(input_details[0]['index'], img_batch)\n",
        "\n",
        "# Run inference\n",
        "interpreter.invoke()\n",
        "\n",
        "# Get the output tensor\n",
        "output_details = interpreter.get_output_details()\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "predicted_class_index = np.argmax(output_data)\n",
        "print(\"Predicted class index:\", predicted_class_index)"
      ],
      "metadata": {
        "id": "F4CGKQY-CbZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be69100a-8479-4210-9499-82bf4fcefb97"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class index: 319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Generator for running inference on large number of images**"
      ],
      "metadata": {
        "id": "0cVQJWtUVhU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load and preprocess the ImageNet validation dataset\n",
        "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    '/content/Small-ImageNet-Validation-Dataset-1000-Classes/ILSVRC2012_img_val_subset',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=1,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        "    classes=[str(i) for i in range(0, 1000)]  # Use category names as classes\n",
        ")\n",
        "\n",
        "# Print class indices\n",
        "print(\"Class Indices:\", validation_generator.class_indices)\n",
        "\n",
        "# Print the number of images\n",
        "num_images = validation_generator.samples\n",
        "print(\"Number of images:\", num_images)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6SFNTke6N9l",
        "outputId": "cda253f8-0c67-4ab8-c065-9f97e358034c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5000 images belonging to 1000 classes.\n",
            "Class Indices: {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14, '15': 15, '16': 16, '17': 17, '18': 18, '19': 19, '20': 20, '21': 21, '22': 22, '23': 23, '24': 24, '25': 25, '26': 26, '27': 27, '28': 28, '29': 29, '30': 30, '31': 31, '32': 32, '33': 33, '34': 34, '35': 35, '36': 36, '37': 37, '38': 38, '39': 39, '40': 40, '41': 41, '42': 42, '43': 43, '44': 44, '45': 45, '46': 46, '47': 47, '48': 48, '49': 49, '50': 50, '51': 51, '52': 52, '53': 53, '54': 54, '55': 55, '56': 56, '57': 57, '58': 58, '59': 59, '60': 60, '61': 61, '62': 62, '63': 63, '64': 64, '65': 65, '66': 66, '67': 67, '68': 68, '69': 69, '70': 70, '71': 71, '72': 72, '73': 73, '74': 74, '75': 75, '76': 76, '77': 77, '78': 78, '79': 79, '80': 80, '81': 81, '82': 82, '83': 83, '84': 84, '85': 85, '86': 86, '87': 87, '88': 88, '89': 89, '90': 90, '91': 91, '92': 92, '93': 93, '94': 94, '95': 95, '96': 96, '97': 97, '98': 98, '99': 99, '100': 100, '101': 101, '102': 102, '103': 103, '104': 104, '105': 105, '106': 106, '107': 107, '108': 108, '109': 109, '110': 110, '111': 111, '112': 112, '113': 113, '114': 114, '115': 115, '116': 116, '117': 117, '118': 118, '119': 119, '120': 120, '121': 121, '122': 122, '123': 123, '124': 124, '125': 125, '126': 126, '127': 127, '128': 128, '129': 129, '130': 130, '131': 131, '132': 132, '133': 133, '134': 134, '135': 135, '136': 136, '137': 137, '138': 138, '139': 139, '140': 140, '141': 141, '142': 142, '143': 143, '144': 144, '145': 145, '146': 146, '147': 147, '148': 148, '149': 149, '150': 150, '151': 151, '152': 152, '153': 153, '154': 154, '155': 155, '156': 156, '157': 157, '158': 158, '159': 159, '160': 160, '161': 161, '162': 162, '163': 163, '164': 164, '165': 165, '166': 166, '167': 167, '168': 168, '169': 169, '170': 170, '171': 171, '172': 172, '173': 173, '174': 174, '175': 175, '176': 176, '177': 177, '178': 178, '179': 179, '180': 180, '181': 181, '182': 182, '183': 183, '184': 184, '185': 185, '186': 186, '187': 187, '188': 188, '189': 189, '190': 190, '191': 191, '192': 192, '193': 193, '194': 194, '195': 195, '196': 196, '197': 197, '198': 198, '199': 199, '200': 200, '201': 201, '202': 202, '203': 203, '204': 204, '205': 205, '206': 206, '207': 207, '208': 208, '209': 209, '210': 210, '211': 211, '212': 212, '213': 213, '214': 214, '215': 215, '216': 216, '217': 217, '218': 218, '219': 219, '220': 220, '221': 221, '222': 222, '223': 223, '224': 224, '225': 225, '226': 226, '227': 227, '228': 228, '229': 229, '230': 230, '231': 231, '232': 232, '233': 233, '234': 234, '235': 235, '236': 236, '237': 237, '238': 238, '239': 239, '240': 240, '241': 241, '242': 242, '243': 243, '244': 244, '245': 245, '246': 246, '247': 247, '248': 248, '249': 249, '250': 250, '251': 251, '252': 252, '253': 253, '254': 254, '255': 255, '256': 256, '257': 257, '258': 258, '259': 259, '260': 260, '261': 261, '262': 262, '263': 263, '264': 264, '265': 265, '266': 266, '267': 267, '268': 268, '269': 269, '270': 270, '271': 271, '272': 272, '273': 273, '274': 274, '275': 275, '276': 276, '277': 277, '278': 278, '279': 279, '280': 280, '281': 281, '282': 282, '283': 283, '284': 284, '285': 285, '286': 286, '287': 287, '288': 288, '289': 289, '290': 290, '291': 291, '292': 292, '293': 293, '294': 294, '295': 295, '296': 296, '297': 297, '298': 298, '299': 299, '300': 300, '301': 301, '302': 302, '303': 303, '304': 304, '305': 305, '306': 306, '307': 307, '308': 308, '309': 309, '310': 310, '311': 311, '312': 312, '313': 313, '314': 314, '315': 315, '316': 316, '317': 317, '318': 318, '319': 319, '320': 320, '321': 321, '322': 322, '323': 323, '324': 324, '325': 325, '326': 326, '327': 327, '328': 328, '329': 329, '330': 330, '331': 331, '332': 332, '333': 333, '334': 334, '335': 335, '336': 336, '337': 337, '338': 338, '339': 339, '340': 340, '341': 341, '342': 342, '343': 343, '344': 344, '345': 345, '346': 346, '347': 347, '348': 348, '349': 349, '350': 350, '351': 351, '352': 352, '353': 353, '354': 354, '355': 355, '356': 356, '357': 357, '358': 358, '359': 359, '360': 360, '361': 361, '362': 362, '363': 363, '364': 364, '365': 365, '366': 366, '367': 367, '368': 368, '369': 369, '370': 370, '371': 371, '372': 372, '373': 373, '374': 374, '375': 375, '376': 376, '377': 377, '378': 378, '379': 379, '380': 380, '381': 381, '382': 382, '383': 383, '384': 384, '385': 385, '386': 386, '387': 387, '388': 388, '389': 389, '390': 390, '391': 391, '392': 392, '393': 393, '394': 394, '395': 395, '396': 396, '397': 397, '398': 398, '399': 399, '400': 400, '401': 401, '402': 402, '403': 403, '404': 404, '405': 405, '406': 406, '407': 407, '408': 408, '409': 409, '410': 410, '411': 411, '412': 412, '413': 413, '414': 414, '415': 415, '416': 416, '417': 417, '418': 418, '419': 419, '420': 420, '421': 421, '422': 422, '423': 423, '424': 424, '425': 425, '426': 426, '427': 427, '428': 428, '429': 429, '430': 430, '431': 431, '432': 432, '433': 433, '434': 434, '435': 435, '436': 436, '437': 437, '438': 438, '439': 439, '440': 440, '441': 441, '442': 442, '443': 443, '444': 444, '445': 445, '446': 446, '447': 447, '448': 448, '449': 449, '450': 450, '451': 451, '452': 452, '453': 453, '454': 454, '455': 455, '456': 456, '457': 457, '458': 458, '459': 459, '460': 460, '461': 461, '462': 462, '463': 463, '464': 464, '465': 465, '466': 466, '467': 467, '468': 468, '469': 469, '470': 470, '471': 471, '472': 472, '473': 473, '474': 474, '475': 475, '476': 476, '477': 477, '478': 478, '479': 479, '480': 480, '481': 481, '482': 482, '483': 483, '484': 484, '485': 485, '486': 486, '487': 487, '488': 488, '489': 489, '490': 490, '491': 491, '492': 492, '493': 493, '494': 494, '495': 495, '496': 496, '497': 497, '498': 498, '499': 499, '500': 500, '501': 501, '502': 502, '503': 503, '504': 504, '505': 505, '506': 506, '507': 507, '508': 508, '509': 509, '510': 510, '511': 511, '512': 512, '513': 513, '514': 514, '515': 515, '516': 516, '517': 517, '518': 518, '519': 519, '520': 520, '521': 521, '522': 522, '523': 523, '524': 524, '525': 525, '526': 526, '527': 527, '528': 528, '529': 529, '530': 530, '531': 531, '532': 532, '533': 533, '534': 534, '535': 535, '536': 536, '537': 537, '538': 538, '539': 539, '540': 540, '541': 541, '542': 542, '543': 543, '544': 544, '545': 545, '546': 546, '547': 547, '548': 548, '549': 549, '550': 550, '551': 551, '552': 552, '553': 553, '554': 554, '555': 555, '556': 556, '557': 557, '558': 558, '559': 559, '560': 560, '561': 561, '562': 562, '563': 563, '564': 564, '565': 565, '566': 566, '567': 567, '568': 568, '569': 569, '570': 570, '571': 571, '572': 572, '573': 573, '574': 574, '575': 575, '576': 576, '577': 577, '578': 578, '579': 579, '580': 580, '581': 581, '582': 582, '583': 583, '584': 584, '585': 585, '586': 586, '587': 587, '588': 588, '589': 589, '590': 590, '591': 591, '592': 592, '593': 593, '594': 594, '595': 595, '596': 596, '597': 597, '598': 598, '599': 599, '600': 600, '601': 601, '602': 602, '603': 603, '604': 604, '605': 605, '606': 606, '607': 607, '608': 608, '609': 609, '610': 610, '611': 611, '612': 612, '613': 613, '614': 614, '615': 615, '616': 616, '617': 617, '618': 618, '619': 619, '620': 620, '621': 621, '622': 622, '623': 623, '624': 624, '625': 625, '626': 626, '627': 627, '628': 628, '629': 629, '630': 630, '631': 631, '632': 632, '633': 633, '634': 634, '635': 635, '636': 636, '637': 637, '638': 638, '639': 639, '640': 640, '641': 641, '642': 642, '643': 643, '644': 644, '645': 645, '646': 646, '647': 647, '648': 648, '649': 649, '650': 650, '651': 651, '652': 652, '653': 653, '654': 654, '655': 655, '656': 656, '657': 657, '658': 658, '659': 659, '660': 660, '661': 661, '662': 662, '663': 663, '664': 664, '665': 665, '666': 666, '667': 667, '668': 668, '669': 669, '670': 670, '671': 671, '672': 672, '673': 673, '674': 674, '675': 675, '676': 676, '677': 677, '678': 678, '679': 679, '680': 680, '681': 681, '682': 682, '683': 683, '684': 684, '685': 685, '686': 686, '687': 687, '688': 688, '689': 689, '690': 690, '691': 691, '692': 692, '693': 693, '694': 694, '695': 695, '696': 696, '697': 697, '698': 698, '699': 699, '700': 700, '701': 701, '702': 702, '703': 703, '704': 704, '705': 705, '706': 706, '707': 707, '708': 708, '709': 709, '710': 710, '711': 711, '712': 712, '713': 713, '714': 714, '715': 715, '716': 716, '717': 717, '718': 718, '719': 719, '720': 720, '721': 721, '722': 722, '723': 723, '724': 724, '725': 725, '726': 726, '727': 727, '728': 728, '729': 729, '730': 730, '731': 731, '732': 732, '733': 733, '734': 734, '735': 735, '736': 736, '737': 737, '738': 738, '739': 739, '740': 740, '741': 741, '742': 742, '743': 743, '744': 744, '745': 745, '746': 746, '747': 747, '748': 748, '749': 749, '750': 750, '751': 751, '752': 752, '753': 753, '754': 754, '755': 755, '756': 756, '757': 757, '758': 758, '759': 759, '760': 760, '761': 761, '762': 762, '763': 763, '764': 764, '765': 765, '766': 766, '767': 767, '768': 768, '769': 769, '770': 770, '771': 771, '772': 772, '773': 773, '774': 774, '775': 775, '776': 776, '777': 777, '778': 778, '779': 779, '780': 780, '781': 781, '782': 782, '783': 783, '784': 784, '785': 785, '786': 786, '787': 787, '788': 788, '789': 789, '790': 790, '791': 791, '792': 792, '793': 793, '794': 794, '795': 795, '796': 796, '797': 797, '798': 798, '799': 799, '800': 800, '801': 801, '802': 802, '803': 803, '804': 804, '805': 805, '806': 806, '807': 807, '808': 808, '809': 809, '810': 810, '811': 811, '812': 812, '813': 813, '814': 814, '815': 815, '816': 816, '817': 817, '818': 818, '819': 819, '820': 820, '821': 821, '822': 822, '823': 823, '824': 824, '825': 825, '826': 826, '827': 827, '828': 828, '829': 829, '830': 830, '831': 831, '832': 832, '833': 833, '834': 834, '835': 835, '836': 836, '837': 837, '838': 838, '839': 839, '840': 840, '841': 841, '842': 842, '843': 843, '844': 844, '845': 845, '846': 846, '847': 847, '848': 848, '849': 849, '850': 850, '851': 851, '852': 852, '853': 853, '854': 854, '855': 855, '856': 856, '857': 857, '858': 858, '859': 859, '860': 860, '861': 861, '862': 862, '863': 863, '864': 864, '865': 865, '866': 866, '867': 867, '868': 868, '869': 869, '870': 870, '871': 871, '872': 872, '873': 873, '874': 874, '875': 875, '876': 876, '877': 877, '878': 878, '879': 879, '880': 880, '881': 881, '882': 882, '883': 883, '884': 884, '885': 885, '886': 886, '887': 887, '888': 888, '889': 889, '890': 890, '891': 891, '892': 892, '893': 893, '894': 894, '895': 895, '896': 896, '897': 897, '898': 898, '899': 899, '900': 900, '901': 901, '902': 902, '903': 903, '904': 904, '905': 905, '906': 906, '907': 907, '908': 908, '909': 909, '910': 910, '911': 911, '912': 912, '913': 913, '914': 914, '915': 915, '916': 916, '917': 917, '918': 918, '919': 919, '920': 920, '921': 921, '922': 922, '923': 923, '924': 924, '925': 925, '926': 926, '927': 927, '928': 928, '929': 929, '930': 930, '931': 931, '932': 932, '933': 933, '934': 934, '935': 935, '936': 936, '937': 937, '938': 938, '939': 939, '940': 940, '941': 941, '942': 942, '943': 943, '944': 944, '945': 945, '946': 946, '947': 947, '948': 948, '949': 949, '950': 950, '951': 951, '952': 952, '953': 953, '954': 954, '955': 955, '956': 956, '957': 957, '958': 958, '959': 959, '960': 960, '961': 961, '962': 962, '963': 963, '964': 964, '965': 965, '966': 966, '967': 967, '968': 968, '969': 969, '970': 970, '971': 971, '972': 972, '973': 973, '974': 974, '975': 975, '976': 976, '977': 977, '978': 978, '979': 979, '980': 980, '981': 981, '982': 982, '983': 983, '984': 984, '985': 985, '986': 986, '987': 987, '988': 988, '989': 989, '990': 990, '991': 991, '992': 992, '993': 993, '994': 994, '995': 995, '996': 996, '997': 997, '998': 998, '999': 999}\n",
            "Number of images: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparing original model vs tflite's accuracy**"
      ],
      "metadata": {
        "id": "vfJEdx851KJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorFlow Lite model\n",
        "interpreter = tf.lite.Interpreter(model_path='/content/vgg19_tfl.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Initialize variables to store correct predictions\n",
        "vgg_correct = 0\n",
        "tflite_correct = 0\n",
        "total_samples = 0\n",
        "desired_samples = 100 # Number of images from validation set you want to evaluate\n",
        "\n",
        "# Iterate over the validation dataset and make predictions\n",
        "for x_batch, y_batch in validation_generator:\n",
        "    batch_size = len(x_batch)\n",
        "    total_samples += batch_size\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # VGG16 predictions\n",
        "        vgg_prediction = np.argmax(model.predict(np.expand_dims(x_batch[i], axis=0)))\n",
        "        vgg_correct += int(vgg_prediction == np.argmax(y_batch[i]))\n",
        "\n",
        "        # TensorFlow Lite model predictions\n",
        "        # Preprocess input image (same as for original model)\n",
        "        img_batch = np.expand_dims(x_batch[i], axis=0)\n",
        "        # x_batch_preprocessed = preprocess_input(img_batch)\n",
        "        img_batch = (np.round((img_batch/1.0774157047271729)-13)-128).astype(np.uint8)\n",
        "\n",
        "        # Set the input tensor for the TensorFlow Lite model\n",
        "        input_details = interpreter.get_input_details()\n",
        "        interpreter.set_tensor(input_details[0]['index'], img_batch)\n",
        "\n",
        "        # Run inference\n",
        "        interpreter.invoke()\n",
        "\n",
        "        # Get the output tensor\n",
        "        output_details = interpreter.get_output_details()\n",
        "\n",
        "        # Get output tensor from TensorFlow Lite model\n",
        "        tflite_prediction = np.argmax(interpreter.get_tensor(output_details[0]['index']))\n",
        "        tflite_correct += int(tflite_prediction == np.argmax(y_batch[i]))\n",
        "        # print(f'model: {tflite_prediction} actual: {np.argmax(y_batch[i])}')\n",
        "    if total_samples >= desired_samples:\n",
        "        break\n",
        "\n",
        "# Calculate accuracies\n",
        "vgg_accuracy = vgg_correct / total_samples\n",
        "tflite_accuracy = tflite_correct / total_samples\n",
        "\n",
        "print(\"VGG19 Model's Accuracy:\", vgg_accuracy)\n",
        "print(\"TFLite Model's Accuracy:\", tflite_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RID3nvy20HCx",
        "outputId": "32f2cb69-09ad-407b-85aa-2664f9175242"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 715ms/step\n",
            "1/1 [==============================] - 1s 692ms/step\n",
            "1/1 [==============================] - 1s 632ms/step\n",
            "1/1 [==============================] - 1s 647ms/step\n",
            "1/1 [==============================] - 1s 642ms/step\n",
            "1/1 [==============================] - 1s 630ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 638ms/step\n",
            "1/1 [==============================] - 1s 643ms/step\n",
            "1/1 [==============================] - 1s 640ms/step\n",
            "1/1 [==============================] - 1s 623ms/step\n",
            "1/1 [==============================] - 1s 649ms/step\n",
            "1/1 [==============================] - 1s 652ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 654ms/step\n",
            "1/1 [==============================] - 1s 640ms/step\n",
            "1/1 [==============================] - 1s 650ms/step\n",
            "1/1 [==============================] - 1s 650ms/step\n",
            "1/1 [==============================] - 1s 634ms/step\n",
            "1/1 [==============================] - 1s 707ms/step\n",
            "1/1 [==============================] - 1s 991ms/step\n",
            "1/1 [==============================] - 1s 639ms/step\n",
            "1/1 [==============================] - 1s 624ms/step\n",
            "1/1 [==============================] - 1s 641ms/step\n",
            "1/1 [==============================] - 1s 643ms/step\n",
            "1/1 [==============================] - 1s 637ms/step\n",
            "1/1 [==============================] - 1s 863ms/step\n",
            "1/1 [==============================] - 1s 881ms/step\n",
            "1/1 [==============================] - 1s 643ms/step\n",
            "1/1 [==============================] - 1s 650ms/step\n",
            "1/1 [==============================] - 1s 630ms/step\n",
            "1/1 [==============================] - 1s 642ms/step\n",
            "1/1 [==============================] - 1s 645ms/step\n",
            "1/1 [==============================] - 1s 894ms/step\n",
            "1/1 [==============================] - 1s 810ms/step\n",
            "1/1 [==============================] - 1s 628ms/step\n",
            "1/1 [==============================] - 1s 638ms/step\n",
            "1/1 [==============================] - 1s 673ms/step\n",
            "1/1 [==============================] - 1s 650ms/step\n",
            "1/1 [==============================] - 1s 647ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 645ms/step\n",
            "1/1 [==============================] - 1s 643ms/step\n",
            "1/1 [==============================] - 1s 625ms/step\n",
            "1/1 [==============================] - 1s 636ms/step\n",
            "1/1 [==============================] - 1s 624ms/step\n",
            "1/1 [==============================] - 1s 626ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 632ms/step\n",
            "1/1 [==============================] - 1s 624ms/step\n",
            "1/1 [==============================] - 1s 642ms/step\n",
            "1/1 [==============================] - 1s 646ms/step\n",
            "1/1 [==============================] - 1s 664ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 653ms/step\n",
            "1/1 [==============================] - 1s 672ms/step\n",
            "1/1 [==============================] - 1s 651ms/step\n",
            "1/1 [==============================] - 1s 630ms/step\n",
            "1/1 [==============================] - 1s 646ms/step\n",
            "1/1 [==============================] - 1s 641ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 632ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 640ms/step\n",
            "1/1 [==============================] - 1s 637ms/step\n",
            "1/1 [==============================] - 1s 623ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 625ms/step\n",
            "1/1 [==============================] - 1s 640ms/step\n",
            "1/1 [==============================] - 1s 646ms/step\n",
            "1/1 [==============================] - 1s 631ms/step\n",
            "1/1 [==============================] - 1s 644ms/step\n",
            "1/1 [==============================] - 1s 660ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 632ms/step\n",
            "1/1 [==============================] - 1s 626ms/step\n",
            "1/1 [==============================] - 1s 639ms/step\n",
            "1/1 [==============================] - 1s 625ms/step\n",
            "1/1 [==============================] - 1s 631ms/step\n",
            "1/1 [==============================] - 1s 711ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 665ms/step\n",
            "1/1 [==============================] - 1s 701ms/step\n",
            "1/1 [==============================] - 1s 690ms/step\n",
            "1/1 [==============================] - 1s 670ms/step\n",
            "1/1 [==============================] - 1s 695ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 667ms/step\n",
            "1/1 [==============================] - 1s 711ms/step\n",
            "1/1 [==============================] - 1s 662ms/step\n",
            "1/1 [==============================] - 1s 665ms/step\n",
            "1/1 [==============================] - 1s 645ms/step\n",
            "1/1 [==============================] - 1s 762ms/step\n",
            "1/1 [==============================] - 1s 813ms/step\n",
            "1/1 [==============================] - 1s 712ms/step\n",
            "1/1 [==============================] - 1s 716ms/step\n",
            "1/1 [==============================] - 1s 802ms/step\n",
            "1/1 [==============================] - 1s 736ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "VGG19 Model's Accuracy: 0.69\n",
            "TFLite Model's Accuracy: 0.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion: We have successfully converted the model to its tflte integer version without much loss in accuracy**"
      ],
      "metadata": {
        "id": "Jw8H1G2jpb2f"
      }
    }
  ]
}